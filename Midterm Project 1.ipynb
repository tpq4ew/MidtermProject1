{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab05ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import certifi\n",
    "import mysql.connector\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "print(f\"Running SQL Alchemy Version: {sqlalchemy.__version__}\")\n",
    "print(f\"Running PyMongo Version: {pymongo.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8b0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data from CSV1\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'UnderwearData')\n",
    "data_file = os.path.join(data_dir, 'suppliers.csv')\n",
    "\n",
    "df_suppliers = pd.read_csv(data_file, header=0, index_col=0)\n",
    "df_suppliers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcbefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data from CSV2\n",
    "\n",
    "data_dir2 = os.path.join(os.getcwd(), 'UnderwearData')\n",
    "data_file2 = os.path.join(data_dir, 'purchase_orders.csv')\n",
    "\n",
    "df_purhcase_orders = pd.read_csv(data_file2, header=0, index_col=0)\n",
    "df_purhcase_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring and Assigning Connection Variables for the MongoDB Server, the mySQL server, & Databases \n",
    "\n",
    "mysql_args = {\n",
    "    \"uid\" : \"root\",\n",
    "    \"pwd\" : \"Passw0rd123\",\n",
    "    \"hostname\" : \"localhost\",\n",
    "    \"dbname\" : \"UnderwearData\",\n",
    "    \"dst_dbname\" : \"UnderwearData2\"\n",
    "}\n",
    "\n",
    "\n",
    "# The 'cluster_location' must either be \"atlas\" or \"local\".\n",
    "mongodb_args = {\n",
    "    \"user_name\" : \"\",\n",
    "    \"password\" : \"password\",\n",
    "    \"cluster_name\" : \"cluster_underwearData\",\n",
    "    \"cluster_subnet\" : \"123456\",\n",
    "    \"cluster_location\" : \"local\", # \"local\"\n",
    "    \"db_name\" : \"underwear_data\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functions for getting data from and setting data into databases\n",
    "\n",
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        connection.execute(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\")\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the cluster_location parameter.\")\n",
    "    \n",
    "    else:\n",
    "        if args[\"cluster_location\"] == \"atlas\":\n",
    "            connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "            connect_str += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "            client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "            \n",
    "        elif args[\"cluster_location\"] == \"local\":\n",
    "            client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        \n",
    "    return client\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_mongo_collections(mongo_client, db_name, data_directory, json_files):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a9c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new Data Warehouse databse and switching the connection context \n",
    "\n",
    "conn_str = f\"mysql+pymysql://{\"uid\"}:{\"pwd\"}@{\"hostname\"}\"\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "connection = sqlEngine.connect()\n",
    "\n",
    "connection.execute(f\"DROP DATABASE IF EXISTS `{dst_dbname}`;\")\n",
    "connection.execute(f\"CREATE DATABASE `{dst_dbname}`;\")\n",
    "connection.execute(f\"USE {dst_dbname};\")\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting and Cleaning data from dimension table from SQL database \n",
    "sql_products = \"SELECT * FROM underweardata.dim_products;\"\n",
    "df_products = get_dataframe(user_id, pwd, host_name, src_dbname, sql_dim_products)\n",
    "df_products.drop(['Color'], axis = 1, inplace=True)\n",
    "df_products.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a3126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting and Cleaning data from dimension table from SQL database\n",
    "sql_inventory_transactions = \"SELECT * FROM underweardata.dim_inventory_transactions;\"\n",
    "df_inventory_transactions = get_dataframe(user_id, pwd, host_name, src_dbname, sql_dim_inventory_transactions)\n",
    "df_inventory_transactions.drop(['MissingID','UnitPurchasePrice','QuantityMissing'], axis = 1, inplace =True)\n",
    "df_inventory_transactions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07147c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting data from date dimension table created in SQL database\n",
    "sql_dim_date = \"SELECT * FROM underweardata.dim_date;\"\n",
    "df_dim_date = get_dataframe(user_id, pwd, host_name, src_dbname, sql_dim_date)\n",
    "df_dim_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating fact table \n",
    "df_fact_orders = pd.merge(df_purchase_orders, df_inventory_transactions, on = 'PurchaseOrderID', how = 'right')\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6953f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d17afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the 'primary key' from suppliers dimension table\n",
    "sql_dim_suppliers = \"SELECT SupplierID FROM UnderwearData.suppliers;\"\n",
    "df_dim_suppliers = get_sql_dataframe(sql_dim_suppliers, **mysql_args)\n",
    "df_dim_suppliers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42370a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining fact table with suppliers dimension table\n",
    "df_fact_inventory = pd.merge(df_dim_suppliers, df_fact_orders, on = 'SupplierID', how = 'inner')\n",
    "df_fact_inventory.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e39ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading transformed dataframes into new datawarehouse\n",
    "dataframe = df_products\n",
    "table_name = 'dim_products'\n",
    "primary_key = 'ProductID'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9beab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading transformed dataframes into new datawarehouse\n",
    "dataframe = df_inventory_transactions\n",
    "table_name = 'dim_inventory_transactions'\n",
    "primary_key = 'TransactionID'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9e78a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting fact table back into SQL data warehouse \n",
    "dataframe = df_fact_inventory\n",
    "table_name = 'fact_inventory_transactions'\n",
    "primary_key = 'SupplierID'\n",
    "db_operation = \"insert\"\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98da069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "sql_fact_inventory = \"\"\"\n",
    "\n",
    "SUM(products.PurchasePrice) AS 'Total Purchase Price' \n",
    "FROM UderwearData2.fact_inventory_transactions AS po \n",
    "INNER JOIN UnderwearData2.dim_products AS s\n",
    "ON po.ProductID = s.ProductID\n",
    "GROUP BY s.ProductID \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18072dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL Query\n",
    "sql_fact_inventory = \"\"\"\n",
    "\n",
    "AVERAGE(products.PurchasePrice) AS 'Total Purchase Price' \n",
    "FROM UderwearData2.fact_inventory_transactions AS po \n",
    "INNER JOIN UnderwearData2.dim_products AS s\n",
    "ON po.ProductID = s.ProductID\n",
    "GROUP BY s.ProductID \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
